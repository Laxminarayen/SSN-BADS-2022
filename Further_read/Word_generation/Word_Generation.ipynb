{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from numpy import array\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical \n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to read the text data into memory \n",
    "import re\n",
    "\n",
    "def load_doc(filename):\n",
    "    #open the file in read only mode \n",
    "    file = open(filename,'r')\n",
    "    #read all the text \n",
    "    text = file.read()\n",
    "    #close the file\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_filename = \"republic_sequences.txt\"\n",
    "doc = load_doc(in_filename)\n",
    "doc = doc[:100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_doc(doc):\n",
    "    #remove all the punctuations\n",
    "    doc = re.sub(r'[^\\w\\s]','',doc)\n",
    "    #word tokenization\n",
    "    tokens = doc.split()\n",
    "    #remove anything other than alphanumeric words\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    #convert it to lower\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean document \n",
    "tokens = clean_doc(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['introduction', 'and', 'analysis', 'the', 'republic', 'of', 'plato', 'is', 'the', 'longest', 'of', 'his', 'works', 'with', 'the', 'exception', 'of', 'the', 'laws', 'and', 'is', 'certainly', 'the', 'greatest', 'of', 'them', 'there', 'are', 'nearer', 'approaches', 'to', 'modern', 'metaphysics', 'in', 'the', 'philebus', 'and', 'in', 'the', 'sophist', 'the', 'politicus', 'or', 'statesman', 'is', 'more', 'ideal', 'the', 'form', 'and', 'institutions', 'and', 'analysis', 'the', 'republic', 'of', 'plato', 'is', 'the', 'longest', 'of', 'his', 'works', 'with', 'the', 'exception', 'of', 'the', 'laws', 'and', 'is', 'certainly', 'the', 'greatest', 'of', 'them', 'there', 'are', 'nearer', 'approaches', 'to', 'modern', 'metaphysics', 'in', 'the', 'philebus', 'and', 'in', 'the', 'sophist', 'the', 'politicus', 'or', 'statesman', 'is', 'more', 'ideal', 'the', 'form', 'and']\n"
     ]
    }
   ],
   "source": [
    "print(tokens[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X                                   Y\n",
    "--------------------000000000000000 -\n",
    "--------------000000000000000000000 -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total no. of sequences 17676\n"
     ]
    }
   ],
   "source": [
    "#Train - 50 words - Predict the 51st \n",
    "#First 50 - X, 51st word - y\n",
    "\n",
    "length = 50+1\n",
    "sequences = list()\n",
    "# 51 - len(no words in the corpus)\n",
    "for i in range(length,len(tokens)): #= [0-51] - [i-length,length]\n",
    "    #Make sequence of tokens\n",
    "    #First i -length = 0,51\n",
    "    seq = tokens[i-length:i]\n",
    "    line = \" \".join(seq)\n",
    "    sequences.append(line)\n",
    "\n",
    "print(\"The total no. of sequences\", len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each sequence is of length: 51\n"
     ]
    }
   ],
   "source": [
    "print(\"Each sequence is of length:\",len(sequences[0].split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoding my sequence if words\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_index)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = array(sequences)\n",
    "X,y = sequences[:,:-1],sequences[:,-1]\n",
    "y = to_categorical(y,num_classes = vocab_size)\n",
    "seq_length = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "205"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 50, 50)            10250     \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 50, 100)           60400     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 205)               20705     \n",
      "=================================================================\n",
      "Total params: 181,855\n",
      "Trainable params: 181,855\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Define the model \n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size,50,input_length = seq_length))\n",
    "model.add(LSTM(100,return_sequences = True))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(100,activation= 'relu'))\n",
    "model.add(Dense(vocab_size,activation = 'softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17676 samples\n",
      "Epoch 1/10\n",
      "17676/17676 [==============================] - 120s 7ms/sample - loss: 4.7492 - accuracy: 0.0979\n",
      "Epoch 2/10\n",
      "17676/17676 [==============================] - 108s 6ms/sample - loss: 4.4064 - accuracy: 0.1099\n",
      "Epoch 3/10\n",
      "17676/17676 [==============================] - 100s 6ms/sample - loss: 3.3104 - accuracy: 0.2094\n",
      "Epoch 4/10\n",
      "17676/17676 [==============================] - 99s 6ms/sample - loss: 2.2327 - accuracy: 0.4546\n",
      "Epoch 5/10\n",
      "17676/17676 [==============================] - 101s 6ms/sample - loss: 1.5266 - accuracy: 0.6912\n",
      "Epoch 6/10\n",
      "17676/17676 [==============================] - 99s 6ms/sample - loss: 1.1192 - accuracy: 0.8228\n",
      "Epoch 7/10\n",
      "17676/17676 [==============================] - 100s 6ms/sample - loss: 0.8723 - accuracy: 0.8926\n",
      "Epoch 8/10\n",
      "17676/17676 [==============================] - 101s 6ms/sample - loss: 0.7115 - accuracy: 0.9222\n",
      "Epoch 9/10\n",
      "17676/17676 [==============================] - 102s 6ms/sample - loss: 0.5950 - accuracy: 0.9363\n",
      "Epoch 10/10\n",
      "17676/17676 [==============================] - 102s 6ms/sample - loss: 0.5213 - accuracy: 0.9435\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fdb246669e8>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss = 'categorical_crossentropy',optimizer = 'adam',metrics = ['accuracy'])\n",
    "model.fit(X,y,batch_size = 128,epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model \n",
    "from pickle import load \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "#Generating Seq from our Language model \n",
    "def generate_seq(model,tokenizer,seq_length,in_text,n_words):\n",
    "    #results = list()\n",
    "    #Generate a fixed no of words\n",
    "    #results.append(in_text)\n",
    "    for _ in range(n_words):\n",
    "        #encode the text to integers \n",
    "        encoded = tokenizer.texts_to_sequences([in_text])\n",
    "        #truncate or pad \n",
    "        encoded = pad_sequences(encoded,maxlen = seq_length,truncating = 'pre')\n",
    "        #predict probability for each word\n",
    "        yhat = model.predict_classes(encoded,verbose = 0)\n",
    "        #print(yhat)\n",
    "        #map the predicted index to word \n",
    "        out_word = ''\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == yhat:\n",
    "                #print(word)\n",
    "                out_word = word\n",
    "                break\n",
    "        in_text+=\" \"+out_word\n",
    "    return in_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_text = 'Hi I am taking classes at inceptez'\n",
    "\n",
    "generated = generate_seq(model,tokenizer,seq_length,in_text,9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi I am taking classes at inceptez there a deeper who conceived a method of knowledge'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
